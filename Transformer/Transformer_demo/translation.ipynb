{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fbbcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于CSDN博客《从零实现Transformer：中英文翻译实例》完整代码\n",
    "# 依赖：PyTorch >= 2.0, 无需额外安装其他库\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a94eb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16662d2d6d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 固定随机种子，保证复现性\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9af3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 1. 全局常量定义 --------------------------\n",
    "# 特殊符号索引（与博客一致）\n",
    "PAD_IDX = 0    # 填充符\n",
    "BOS_IDX = 1    # 句首符\n",
    "EOS_IDX = 2    # 句尾符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c30613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 2. 数据预处理模块 --------------------------\n",
    "# 2.1 构建玩具级中英文平行语料（已经使用空格进行了分词！！！！！！！）\n",
    "pairs = [\n",
    "    (\"我 有 一个 苹果\", \"i have an apple\"),\n",
    "    (\"我 有 一本 书\", \"i have a book\"),\n",
    "    (\"你 有 一个 苹果\", \"you have an apple\"),\n",
    "    (\"他 有 一个 苹果\", \"he has an apple\"),\n",
    "    (\"她 有 一个 苹果\", \"she has an apple\"),\n",
    "    (\"我们 有 一个 苹果\", \"we have an apple\"),\n",
    "    (\"我 喜欢 苹果\", \"i like apples\"),\n",
    "    (\"我 吃 苹果\", \"i eat apples\"),\n",
    "    (\"你 喜欢 书\", \"you like books\"),\n",
    "    (\"我 喜欢 书\", \"i like books\"),\n",
    "    (\"我 有 两个 苹果\", \"i have two apples\"),\n",
    "    (\"我 有 红色 苹果\", \"i have red apples\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7417b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 构建词表（文本→索引 与 索引→文本）\n",
    "def build_vocab(examples: List[str]) -> Tuple[dict, List[str]]:\n",
    "    \"\"\"\n",
    "    输入：空格分词后的句子列表\n",
    "    输出：stoi（词→索引）、itos（索引→词）\n",
    "    \"\"\"\n",
    "    tokens = set()# 用set自动去重，避免同一词汇多次出现\n",
    "    # 遍历所有句子，提取不重复词汇\n",
    "    for s in examples:\n",
    "        for t in s.split():# 按空格拆分句子，得到单个词汇（如“我”“有”“一个”“苹果”）\n",
    "            tokens.add(t.lower())  # 英文统一小写，中文不影响\n",
    "    # 加入特殊符号，按顺序排序（保证复现性）\n",
    "    itos = [\"<pad>\", \"<bos>\", \"<eos>\"] + sorted(tokens)#与前面的特殊符号拼接起来\n",
    "    stoi = {t: i for i, t in enumerate(itos)}#将词与索引一一对应\n",
    "    return stoi, itos#python的多变量赋值\n",
    "\n",
    "# 拆分中英文句子，分别构建词表\n",
    "src_texts = [p[0] for p in pairs]  # 中文句子列表\n",
    "tgt_texts = [p[1] for p in pairs]  # 英文句子列表\n",
    "SRC_STOI, SRC_ITOS = build_vocab(src_texts)  # 中文词表\n",
    "TGT_STOI, TGT_ITOS = build_vocab(tgt_texts)  # 英文词表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ff0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 句子编码函数（文本→索引序列）\n",
    "def encode_src(s: str) -> List[int]:\n",
    "    \"\"\"编码中文源句子（中文句子→数字索引（源语言编码））\"\"\"\n",
    "    return [SRC_STOI[w.lower()] for w in s.split()]\n",
    "\n",
    "def encode_tgt(s: str) -> List[int]:\n",
    "    \"\"\"编码英文目标句子（英文句子→数字索引（目标语言编码））\"\"\"\n",
    "    return [BOS_IDX] + [TGT_STOI[w.lower()] for w in s.split()] + [EOS_IDX]#把输入的英文句子转换成带特殊标记的数字索引列表\n",
    "\n",
    "def decode_tgt(ids: List[int]) -> str:\n",
    "    \"\"\"解码英文索引序列→文本（数字索引→英文句子（目标语言解码））\"\"\"\n",
    "    special_ids = {PAD_IDX, BOS_IDX, EOS_IDX}\n",
    "    tokens = [TGT_ITOS[id] for id in ids if id not in special_ids]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97a71ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 数据集类（自定义数据集，适配PyTorch DataLoader）\n",
    "@dataclass\n",
    "class Example:\n",
    "    \"\"\"单条样本：源语言序列（中文）、目标语言序列（英文）\"\"\"\n",
    "    src: List[int]  # 中文索引序列（无BOS/EOS）\n",
    "    tgt: List[int]  # 英文索引序列（有BOS/EOS）\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[str, str]]):# 元组1：(中文str, 英文str)\n",
    "        self.data = [Example(encode_src(s), encode_tgt(t)) for s, t in pairs]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Example:\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d0b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 批量数据处理（collate_fn，解决句子长度不一致问题（向最长序列的长度看齐））\n",
    "def collate_fn(batch: List[Example]):\n",
    "    \"\"\"\n",
    "    输入：批量Example样本\n",
    "    输出：src, tgt_in, tgt_out, src_pad_mask, tgt_pad_mask（均为Tensor）\n",
    "    \"\"\"\n",
    "    # 计算批量内最长序列长度（用于填充）\n",
    "    src_max_len = max(len(ex.src) for ex in batch)\n",
    "    tgt_max_len = max(len(ex.tgt) for ex in batch) - 1  # tgt_in比tgt短1（去掉EOS）\n",
    "\n",
    "    src_batch = []\n",
    "    tgt_in_batch = []\n",
    "    tgt_out_batch = []\n",
    "\n",
    "    for ex in batch:#遍历样本中的数据来处理每个序列\n",
    "        # 处理中文源序列（填充到src_max_len）用PAD补长\n",
    "        src = ex.src + [PAD_IDX] * (src_max_len - len(ex.src))\n",
    "        # 处理英文目标序列（Teacher Forcing：tgt_in去掉EOS，tgt_out去掉BOS）解码器训练时，输入和标签需要错开一位（用前一个词预测后一个词）翻译模型的关键技巧\n",
    "        tgt_in = ex.tgt[:-1] + [PAD_IDX] * (tgt_max_len - len(ex.tgt[:-1]))#英文目标输入 [BOS, w1, w2, w3]\n",
    "        tgt_out = ex.tgt[1:] + [PAD_IDX] * (tgt_max_len - len(ex.tgt[1:]))#英文目标输出 [w1, w2, w3, EOS]\n",
    "        \n",
    "        #加入批次列表\n",
    "        src_batch.append(src)\n",
    "        tgt_in_batch.append(tgt_in)\n",
    "        tgt_out_batch.append(tgt_out)\n",
    "\n",
    "    # 转换为Tensor\n",
    "    src = torch.tensor(src_batch, dtype=torch.long)\n",
    "    tgt_in = torch.tensor(tgt_in_batch, dtype=torch.long)\n",
    "    tgt_out = torch.tensor(tgt_out_batch, dtype=torch.long)\n",
    "    # 生成填充掩码（标记哪些位置是PAD，模型需要忽略）\n",
    "    src_pad_mask = src.eq(PAD_IDX) # True表示该位置是PAD（0）\n",
    "    tgt_pad_mask = tgt_in.eq(PAD_IDX)# 解码器输入的掩码\n",
    "\n",
    "    return src, tgt_in, tgt_out, src_pad_mask, tgt_pad_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3eebde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化数据加载器\n",
    "dataset = ToyDataset(pairs)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,# 训练时打乱样本顺序\n",
    "    collate_fn=collate_fn# 使用自定义的批量处理函数（填充、拆分、转张量）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb39bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 3. Transformer核心组件 --------------------------\n",
    "# 3.1 位置编码（正弦余弦编码，让模型知道词在句子中的位置）\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):#注意区分max_len与批次内最长序列长度（填充长度）的区别\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)#防止过拟合\n",
    "        # 预计算位置编码矩阵 (max_len, d_model)全0矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        #生成位置索引并将其扩展为列向量（形状：[0,1,2,...,max_len-1] → 扩展为列向量）//.unsqueeze(1)：在第 1 维（列维度）增加一个维度，将形状变为(max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        #计算衰减因子（随着维度i增大，div_term的值呈指数衰减，使高纬度的位置编码变化更为平缓）\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 填充位置编码矩阵偶数维正弦，奇数维余弦\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # 扩展为(1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)  # 不参与训练的缓冲区（缓冲区会随模型保存 / 加载，但不参与反向传播（即不会被优化器更新），位置编码固定，不需要训练）\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x: (B, L, C) → 加位置编码后返回（批次大小、序列长度、词向量维度）\"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]#将词向量与位置编码相加，使词向量同时包含位置信息和语义信息\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46762f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 缩放点积注意力（单头，将点积结果缩小到合理范围内）\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Q: (B, H, Lq, Dh) B：批次大小；H：注意力头数；Lq：查询序列长度；Dh：每个头的维度（d_model / H）\n",
    "        K: (B, H, Lk, Dh)\n",
    "        V: (B, H, Lk, Dh)\n",
    "        mask: 可广播到(B, H, Lq, Lk)的布尔掩码（True表示屏蔽）\n",
    "        返回：(B, H, Lq, Dh)\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1) # 获取每个头的维度Dh（Q的最后一个维度）\n",
    "        # 计算注意力分数：QK^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)#避免梯度消失（softmax 在大数值下会趋于饱和，梯度接近 0）\n",
    "        # 应用掩码（屏蔽位填充-∞，softmax后接近0）；屏蔽PAD、解码器中的未来位置\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "        # 计算注意力权重并应用dropout\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # 注意力加权求和（与V相乘）\n",
    "        out = torch.matmul(attn_weights, V)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db09b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 多头注意力（多头注意力将d_model拆分为nhead个d_head，每个头独立学习一种模式）\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % nhead == 0, \"d_model必须能被nhead整除\"\n",
    "        self.d_model = d_model#词向量总维度\n",
    "        self.nhead = nhead#注意力头数\n",
    "        self.d_head = d_model // nhead  # 每个头的维度\n",
    "\n",
    "        # Q、K、V线性变换层（将输入映射到d_model维度，为拆分多头做准备）\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        # 缩放点积注意力\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        # 合并多头并输出投影层\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _shape(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"(B, L, C) → (B, H, L, Dh)：拆分多头\"\"\"# B：批次大小，L：序列长度，C：总维度（d_model），H：头数，Dh：每个头维度\n",
    "        B, L, C = x.shape\n",
    "        return x.view(B, L, self.nhead, self.d_head).transpose(1, 2)\n",
    "\n",
    "    def _merge(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"(B, H, L, Dh) → (B, L, C)：合并多头\"\"\"\n",
    "        B, H, L, Dh = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(B, L, H * Dh)# 交换维度（transpose）后，张量在内存中可能不连续，contiguous()确保内存连续，避免view操作报错\n",
    "\n",
    "    def _build_attn_mask(self, Lq: int, Lk: int, attn_mask: torch.Tensor = None, key_padding_mask: torch.Tensor = None, device: torch.device = None) -> torch.Tensor:\n",
    "        \"\"\"合并因果掩码和填充掩码：将原始的因果掩码和填充掩码转换为符合注意力计算的形状，并合并两种掩码\"\"\"\n",
    "        mask = None\n",
    "        # 处理因果掩码 (Lq, Lk) → (1, 1, Lq, Lk)\n",
    "        if attn_mask is not None:\n",
    "            mask = attn_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        # 处理填充掩码 (B, Lk) → (B, 1, 1, Lk)\n",
    "        if key_padding_mask is not None:\n",
    "            pad_mask = key_padding_mask.to(device).unsqueeze(1).unsqueeze(1)\n",
    "            mask = pad_mask if mask is None else (mask | pad_mask)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: torch.Tensor = None, key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        输入：\n",
    "        - query/key/value：形状均为(B, L, C)（B=批次，L=序列长度，C=d_model）\n",
    "        - attn_mask：因果掩码（可选，解码器用）\n",
    "        - key_padding_mask：填充掩码（可选，所有层用）\n",
    "        输出：形状为(B, L, C)（与输入query形状一致）\n",
    "        \"\"\"\n",
    "        device = query.device\n",
    "        Lq = query.size(1)\n",
    "        Lk = key.size(1)\n",
    "\n",
    "        # 1. 线性变换 + 拆分多头（从总维度→多头维度）\n",
    "        Q = self._shape(self.w_q(query))\n",
    "        K = self._shape(self.w_k(key))\n",
    "        V = self._shape(self.w_v(value))\n",
    "\n",
    "        # 2. 构建合并掩码（适配多头形状）\n",
    "        mask = self._build_attn_mask(Lq, Lk, attn_mask, key_padding_mask, device)\n",
    "\n",
    "        # 3. 计算单头注意力（调用之前实现的ScaledDotProductAttention）\n",
    "        # 每个头独立计算注意力，输出形状：(B, H, Lq, Dh)\n",
    "        attn_out = self.attn(Q, K, V, mask)\n",
    "\n",
    "        # 4. 合并多头 + 输出投影（从多头维度→总维度）\n",
    "        out = self._merge(attn_out)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61145311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 前馈网络（升维→激活→降维）\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, dim_ff) # 第一层全连接：升维（d_model → dim_ff）\n",
    "        self.fc2 = nn.Linear(dim_ff, d_model) # 第二层全连接：降维（dim_ff → d_model）\n",
    "        self.act = nn.ReLU()# 非线性激活函数\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        输入x形状：(B, L, C) → B=批次，L=序列长度，C=d_model\n",
    "        输出形状：(B, L, C) → 与输入形状一致，便于残差连接\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)# 步骤1：升维 → (B, L, dim_ff)\n",
    "        x = self.act(x)# 步骤2：非线性激活 → 引入非线性特征\n",
    "        x = self.dropout(x)# 步骤3：Dropout → 防止过拟合\n",
    "        x = self.fc2(x)# 步骤4：降维 → 回到d_model维度\n",
    "        x = self.dropout(x)# 步骤5：再次Dropout → 进一步抑制过拟合\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da99652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 编码器层（自注意力捕捉全局依赖 + 前馈网络提取局部特征；残差连接 + 层归一化）\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout) # 多头自注意力模块\n",
    "        self.norm1 = nn.LayerNorm(d_model) # 第一层层归一化（用于自注意力之后）\n",
    "        self.ff = PositionwiseFeedForward(d_model, dim_ff, dropout) # 前馈网络模块\n",
    "        self.norm2 = nn.LayerNorm(d_model) # 第二层层归一化（用于前馈网络之后）\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, S, C)\n",
    "        src_key_padding_mask: (B, S)\n",
    "        返回：(B, S, C)\n",
    "        \"\"\"\n",
    "        # 自注意力 + 残差 + 层归一化（Query=Key=Value=x（输入序列自己与自己做注意力））\n",
    "        attn_out = self.self_attn(x, x, x, key_padding_mask=src_key_padding_mask)\n",
    "        # 残差连接（x + 注意力输出）+ 层归一化\n",
    "        x = self.norm1(x + attn_out)\n",
    "        #  前馈网络处理：对每个位置的特征独立做非线性变换\n",
    "        ff_out = self.ff(x)\n",
    "        #  残差连接（x + 前馈输出）+ 层归一化\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc1642af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 编码器（多层EncoderLayer堆叠）\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_ff: int, num_layers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, nhead, dim_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        输入x：(B, S, C) → B=批次，S=序列长度，C=d_model（经过词嵌入+位置编码的序列）\n",
    "        src_key_padding_mask：(B, S) → 填充掩码，用于屏蔽所有层的PAD位置\n",
    "        输出：(B, S, C) → 经过所有层处理后的最终序列表示\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # 让序列依次经过每个编码器层，每层的输出作为下一层的输入\n",
    "            x = layer(x, src_key_padding_mask=src_key_padding_mask)#掩码共享，确保每个层都不会关注PAD填充符\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb3ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 解码器层（双注意力机制，多层堆叠）\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)  # 解码器自注意力\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout)  # 交叉注意力（连接编码器和解码器）\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ff = PositionwiseFeedForward(d_model, dim_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor = None, tgt_key_padding_mask: torch.Tensor = None, memory_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, C) → 解码器输入\n",
    "        memory: (B, S, C) → 编码器输出\n",
    "        返回：(B, T, C)\n",
    "        \"\"\"\n",
    "        # 1. 解码器自注意力（带因果掩码，关注自身已生成序列）\n",
    "        sa_out = self.self_attn(x, x, x, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)\n",
    "        x = self.norm1(x + sa_out)\n",
    "\n",
    "        # 2. 交叉注意力（Q来自解码器，K/V来自编码器）\n",
    "        ca_out = self.cross_attn(x, memory, memory, key_padding_mask=memory_key_padding_mask)\n",
    "        x = self.norm2(x + ca_out)\n",
    "\n",
    "        # 3. 前馈网络\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm3(x + ff_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 3.8 解码器（多层DecoderLayer堆叠）\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_ff: int, num_layers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([  #自动将内部的DecoderLayer注册为模型子模块，确保训练时参数能被优化器更新\n",
    "            DecoderLayer(d_model, nhead, dim_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor = None, tgt_key_padding_mask: torch.Tensor = None, memory_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"x: (B, T, C) → 经过所有解码器层后返回\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006b488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.9 完整Transformer翻译模型（用编码器提取源语言全局特征，用解码器结合源语言特征逐词生成目标语言，最终通过贪心策略输出翻译结果）\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 128, nhead: int = 4, num_encoder_layers: int = 2, num_decoder_layers: int = 2, dim_ff: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # 词嵌入层（指定填充符，不更新梯度）\n",
    "        self.src_tok = nn.Embedding(src_vocab_size, d_model, padding_idx=PAD_IDX)#中文词嵌入\n",
    "        self.tgt_tok = nn.Embedding(tgt_vocab_size, d_model, padding_idx=PAD_IDX)#英文词嵌入\n",
    "        # 位置编码层：为词嵌入注入位置信息（Transformer无顺序感知，需显式添加）\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n",
    "        # 编码器和解码器\n",
    "        self.encoder = Encoder(d_model, nhead, dim_ff, num_encoder_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, nhead, dim_ff, num_decoder_layers, dropout)\n",
    "        # 生成器（将解码器输出的d_model维特征映射到目标词表维度，用于预测下一个词）\n",
    "        self.generator = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def make_subsequent_mask(self, sz: int) -> torch.Tensor:#生成掩码\n",
    "        \"\"\"生成下三角因果掩码（sz×sz），True表示屏蔽未来位置\"\"\"\n",
    "        # torch.triu：生成上三角矩阵（对角线diagonal=1以上为1，以下为0）\n",
    "        # 转换为bool类型后，True表示需要屏蔽的未来位置\n",
    "        return torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt_in: torch.Tensor, src_pad_mask: torch.Tensor, tgt_pad_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        训练阶段前向传播：\n",
    "        src: (B, S) → 中文源序列\n",
    "        tgt_in: (B, T) → 英文解码器输入（含BOS）\n",
    "        返回：(B, T, tgt_vocab_size) → 词表概率分布\n",
    "        \"\"\"\n",
    "        # 1. 词嵌入 + 位置编码（为词汇添加语义和位置信息）\n",
    "        src_emb = self.pos_enc(self.src_tok(src))  # 源语言：索引→词嵌入→加位置编码 → (B, S, d_model)\n",
    "        tgt_emb = self.pos_enc(self.tgt_tok(tgt_in))  # 目标语言：索引→词嵌入→加位置编码 → (B, T, C)\n",
    "\n",
    "        # 2. 编码器编码：编码器处理源语言序列，输出源语言特征表示（memory）\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_pad_mask)  # (B, S, C)\n",
    "        # memory将传递给解码器，作为交叉注意力的K/V\n",
    "        \n",
    "        # 3. 生成解码器因果掩码（适配目标序列长度）\n",
    "        tgt_mask = self.make_subsequent_mask(tgt_in.size(1)).to(src.device)  # (T, T)，确保与输入在同一设备\n",
    "\n",
    "        # 4. 解码器解码：解码器处理目标语言序列，结合memory生成目标语言特征\n",
    "        out = self.decoder(\n",
    "            tgt_emb,# 目标语言嵌入（含位置编码）\n",
    "            memory,# 编码器输出的源语言特征\n",
    "            tgt_mask=tgt_mask,# 因果掩码（屏蔽未来词）\n",
    "            tgt_key_padding_mask=tgt_pad_mask,# 目标序列PAD掩码\n",
    "            memory_key_padding_mask=src_pad_mask# 源序列PAD掩码（交叉注意力屏蔽源PAD）\n",
    "        )  # (B, T, C)\n",
    "\n",
    "        # 5. 映射到词表，得到预测分数\n",
    "        logits = self.generator(out)  # (B, T, tgt_vocab_size)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()#推理时不计算梯度（节省资源）\n",
    "    def greedy_decode(self, src_ids: List[int], max_len: int = 20, device: str = \"cpu\") -> List[int]:\n",
    "        \"\"\"\n",
    "        推理阶段贪心解码（每次生成一个词时，都选择当前概率最高的词）：\n",
    "        src_ids: 中文源序列索引（无BOS/EOS）\n",
    "        返回：英文目标序列索引（含BOS/EOS）\n",
    "        \"\"\"\n",
    "        self.eval()  # 切换评估模式（Dropout等层随机失活）\n",
    "        # 1. 处理源语言序列（添加批次维度，生成PAD掩码）\n",
    "        src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, S)\n",
    "        src_pad_mask = src.eq(PAD_IDX)  # (1, S)\n",
    "\n",
    "        # 2. 编码器编码\n",
    "        src_emb = self.src_tok(src)\n",
    "        src_emb = self.pos_enc(src_emb)\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_pad_mask)  # (1, S, C)\n",
    "\n",
    "        # 3. 初始化解码器输入（从BOS开始）\n",
    "        ys = torch.tensor([[BOS_IDX]], dtype=torch.long, device=device)  # (1, 1) 当前已生成的目标序列前缀\n",
    "\n",
    "        # 4. 逐词生成目标序列（直到生成EOS或达到max_len）\n",
    "        for _ in range(max_len - 1):\n",
    "            # 生成掩码\n",
    "            tgt_pad_mask = ys.eq(PAD_IDX)  # (1, T)\n",
    "            tgt_mask = self.make_subsequent_mask(ys.size(1)).to(device)  # (T, T)\n",
    "\n",
    "            # 解码器前向（输入当前生成的序列前缀）\n",
    "            tgt_emb = self.tgt_tok(ys)\n",
    "            tgt_emb = self.pos_enc(tgt_emb)\n",
    "            out = self.decoder(\n",
    "                tgt_emb,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_pad_mask,\n",
    "                memory_key_padding_mask=src_pad_mask\n",
    "            )  # (1, T, C)\n",
    "\n",
    "            # 贪心选择概率最大的词\n",
    "            logits = self.generator(out[:, -1:, :])  # (1, 1, tgt_vocab_size)\n",
    "            next_token = logits.argmax(-1)  # (1, 1)\n",
    "            next_id = next_token.item() # 转换为Python整数\n",
    "\n",
    "            # 拼接结果\n",
    "            ys = torch.cat([ys, next_token], dim=1)  # (1, T+1)（序列长度+1）\n",
    "\n",
    "            # 遇到EOS停止\n",
    "            if next_id == EOS_IDX:\n",
    "                break\n",
    "\n",
    "        return ys.squeeze(0).tolist()  # 去掉批次维度，返回索引列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d0407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- 4. 模型训练与推理 --------------------------\n",
    "# 4.1 训练配置（博客原参数）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2SeqTransformer(\n",
    "    src_vocab_size=len(SRC_ITOS),\n",
    "    tgt_vocab_size=len(TGT_ITOS),\n",
    "    d_model=6,  # 博客用小维度，加快训练\n",
    "    nhead=3,    # 6能被3整除\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_ff=256,\n",
    "    dropout=0.1\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f16f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数（忽略填充位）和优化器：loss_backword关联criterion与optimizer(数据流向)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) #计算模型预测与真实目标序列的损失（误差），用于指导模型参数更新\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2286a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 辅助函数：测试翻译效果\n",
    "def evaluate_sample(sent: str = \"我 有 一个 苹果\"):\n",
    "    \"\"\"输入中文句子，输出翻译结果\"\"\"\n",
    "    src_ids = encode_src(sent) # 编码（返回索引列表）\n",
    "    pred_ids = model.greedy_decode(src_ids, device=device) # 推理（将中文索引序列转换为英文索引序列）\n",
    "    pred_text = decode_tgt(pred_ids) # 解码（转换为翻译后的英文句子）\n",
    "    print(f\"输入中文：{sent}\")\n",
    "    print(f\"输出英文：{pred_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "184a8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前测试（随机参数，结果无意义）：\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：eat has eat has eat has eat has has\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.3 训练循环（博客原逻辑，800轮易过拟合玩具语料）\n",
    "print(\"训练前测试（随机参数，结果无意义）：\")\n",
    "evaluate_sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbf80e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | 平均损失：3.0474\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：eat has eat has eat has\n",
      "\n",
      "Epoch 005 | 平均损失：3.0091\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：eat has\n",
      "\n",
      "Epoch 010 | 平均损失：2.7799\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：\n",
      "\n",
      "Epoch 015 | 平均损失：2.7534\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：\n",
      "\n",
      "Epoch 020 | 平均损失：2.7303\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：\n",
      "\n",
      "Epoch 025 | 平均损失：2.7083\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：\n",
      "\n",
      "Epoch 030 | 平均损失：2.6955\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 035 | 平均损失：2.6160\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 040 | 平均损失：2.5827\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 045 | 平均损失：2.5210\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 050 | 平均损失：2.6409\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 055 | 平均损失：2.5986\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 060 | 平均损失：2.5364\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 065 | 平均损失：2.5057\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 070 | 平均损失：2.4566\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 075 | 平均损失：2.4549\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 080 | 平均损失：2.3990\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 085 | 平均损失：2.4899\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 090 | 平均损失：2.3931\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 095 | 平均损失：2.3222\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 100 | 平均损失：2.2786\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i\n",
      "\n",
      "Epoch 105 | 平均损失：2.2833\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 110 | 平均损失：2.2984\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 115 | 平均损失：2.2165\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 120 | 平均损失：2.2357\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 125 | 平均损失：2.3500\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 130 | 平均损失：2.2119\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 135 | 平均损失：2.1625\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 140 | 平均损失：2.1640\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 145 | 平均损失：2.1724\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 150 | 平均损失：2.0938\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 155 | 平均损失：2.1566\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 160 | 平均损失：2.1042\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 165 | 平均损失：1.9897\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 170 | 平均损失：2.1172\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 175 | 平均损失：2.0436\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 180 | 平均损失：2.0618\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 185 | 平均损失：2.1852\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 190 | 平均损失：2.0719\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 195 | 平均损失：1.9604\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 200 | 平均损失：1.9115\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 205 | 平均损失：2.0369\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 210 | 平均损失：2.0022\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 215 | 平均损失：2.0145\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i an\n",
      "\n",
      "Epoch 220 | 平均损失：1.9874\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 225 | 平均损失：1.9827\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 230 | 平均损失：1.9457\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 235 | 平均损失：1.8304\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 240 | 平均损失：1.9649\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 245 | 平均损失：2.0170\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 250 | 平均损失：1.7839\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 255 | 平均损失：1.9072\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 260 | 平均损失：1.8568\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 265 | 平均损失：1.9278\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 270 | 平均损失：1.8570\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 275 | 平均损失：1.8784\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 280 | 平均损失：1.8757\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 285 | 平均损失：1.8253\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 290 | 平均损失：1.8088\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 295 | 平均损失：1.6845\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 300 | 平均损失：1.7742\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 305 | 平均损失：1.6900\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 310 | 平均损失：1.7120\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 315 | 平均损失：1.6647\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 320 | 平均损失：1.8395\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 325 | 平均损失：1.8020\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 330 | 平均损失：1.6875\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 335 | 平均损失：1.7053\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 340 | 平均损失：1.6162\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 345 | 平均损失：1.7164\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 350 | 平均损失：1.7119\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 355 | 平均损失：1.7085\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 360 | 平均损失：1.6919\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 365 | 平均损失：1.6241\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 370 | 平均损失：1.6989\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 375 | 平均损失：1.7221\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 380 | 平均损失：1.6105\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 385 | 平均损失：1.5735\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 390 | 平均损失：1.7148\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 395 | 平均损失：1.6815\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 400 | 平均损失：1.6624\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 405 | 平均损失：1.5232\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 410 | 平均损失：1.6499\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 415 | 平均损失：1.5109\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 420 | 平均损失：1.6951\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 425 | 平均损失：1.5124\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an\n",
      "\n",
      "Epoch 430 | 平均损失：1.5363\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 435 | 平均损失：1.4832\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 440 | 平均损失：1.5035\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 445 | 平均损失：1.6549\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 450 | 平均损失：1.6463\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 455 | 平均损失：1.5243\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 460 | 平均损失：1.4075\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 465 | 平均损失：1.5911\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 470 | 平均损失：1.4958\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 475 | 平均损失：1.4702\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 480 | 平均损失：1.4509\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 485 | 平均损失：1.4722\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 490 | 平均损失：1.3849\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 495 | 平均损失：1.4537\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 500 | 平均损失：1.4726\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 505 | 平均损失：1.5304\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 510 | 平均损失：1.5211\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 515 | 平均损失：1.4642\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 520 | 平均损失：1.4416\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 525 | 平均损失：1.4150\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 530 | 平均损失：1.4626\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 535 | 平均损失：1.2996\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 540 | 平均损失：1.4939\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 545 | 平均损失：1.3088\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 550 | 平均损失：1.4150\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 555 | 平均损失：1.2935\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 560 | 平均损失：1.4021\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 565 | 平均损失：1.3108\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 570 | 平均损失：1.4164\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 575 | 平均损失：1.3943\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 580 | 平均损失：1.4042\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 585 | 平均损失：1.2465\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 590 | 平均损失：1.4241\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 595 | 平均损失：1.2214\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 600 | 平均损失：1.2891\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 605 | 平均损失：1.3333\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 610 | 平均损失：1.3689\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 615 | 平均损失：1.2810\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 620 | 平均损失：1.2047\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 625 | 平均损失：1.2801\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 630 | 平均损失：1.3184\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 635 | 平均损失：1.3646\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 640 | 平均损失：1.2456\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 645 | 平均损失：1.3681\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 650 | 平均损失：1.2438\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 655 | 平均损失：1.2689\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 660 | 平均损失：1.1805\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 665 | 平均损失：1.1445\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 670 | 平均损失：1.3193\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 675 | 平均损失：1.3034\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 680 | 平均损失：1.3354\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 685 | 平均损失：1.2326\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 690 | 平均损失：1.2679\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 695 | 平均损失：1.2082\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 700 | 平均损失：1.3210\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 705 | 平均损失：1.2358\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 710 | 平均损失：1.2988\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 715 | 平均损失：1.0865\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 720 | 平均损失：1.2267\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 725 | 平均损失：1.2765\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 730 | 平均损失：1.2003\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 735 | 平均损失：1.2282\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 740 | 平均损失：1.3981\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 745 | 平均损失：1.2050\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 750 | 平均损失：1.1916\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 755 | 平均损失：1.1528\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 760 | 平均损失：1.2630\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 765 | 平均损失：1.1730\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 770 | 平均损失：1.2385\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 775 | 平均损失：1.2293\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 780 | 平均损失：1.1252\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 785 | 平均损失：1.2431\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 790 | 平均损失：1.0725\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 795 | 平均损失：1.1057\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n",
      "Epoch 800 | 平均损失：1.0885\n",
      "输入中文：我 有 一个 苹果\n",
      "输出英文：i have an apple\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 800\n",
    "## 完成每轮的训练：遍历所有训练数据（dataloader），完成 “数据加载→前向传播→损失计算→反向传播→参数更新” 的全流程\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train() # 模型切换为训练模式\n",
    "    total_loss = 0.0 # 用于累加当前轮所有批次的损失\n",
    "    for src, tgt_in, tgt_out, src_pad_mask, tgt_pad_mask in dataloader:\n",
    "        # 数据移到目标设备（CPU），避免设备不匹配\n",
    "        src = src.to(device)\n",
    "        tgt_in = tgt_in.to(device)\n",
    "        tgt_out = tgt_out.to(device)\n",
    "        src_pad_mask = src_pad_mask.to(device)\n",
    "        tgt_pad_mask = tgt_pad_mask.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        logits = model(src, tgt_in, src_pad_mask, tgt_pad_mask)\n",
    "        # 计算损失（展平维度）；criterion调用交叉熵损失函数，计算展平后的预测与标签的损失（自动忽略PAD_IDX位置）\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "        # 反向传播与优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 梯度裁剪，防止梯度爆炸\n",
    "        optimizer.step() # 优化器根据梯度更新模型参数\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 每5轮打印损失并测试\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        avg_loss = total_loss / len(dataloader) # 总损失除以批次数量\n",
    "        print(f\"Epoch {epoch:03d} | 平均损失：{avg_loss:.4f}\")\n",
    "        evaluate_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e3941aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成！测试其他句子：\n",
      "输入中文：我 喜欢 苹果\n",
      "输出英文：i have have apples\n",
      "\n",
      "输入中文：你 有 一本 书\n",
      "输出英文：i have an apple\n",
      "\n",
      "输入中文：他 吃 苹果\n",
      "输出英文：she has an apple\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.4 最终测试（任意输入中文句子）\n",
    "print(\"训练完成！测试其他句子：\")\n",
    "test_sents = [\"我 喜欢 苹果\", \"你 有 一本 书\", \"他 吃 苹果\"]\n",
    "for sent in test_sents:\n",
    "    evaluate_sample(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52323465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在本demo中，主要是为了展示Transformer在翻译中的具体应用，设置的训练集较为简单，所得结果精度不高"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
